{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evalevanto/Indaba-2024-GeoAI-Challenge/blob/main/bootstrap_geoai_challenge_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5wt2-MLIWmr"
      },
      "source": [
        "\n",
        "# Starter Code\n",
        "\n",
        "- [x] Load and run `quantized-4bit LLaVA 1.5 7B`.\n",
        "- [x] Load a dataset with `train` and `test` splits using `datasets`.\n",
        "- [x] Create a `pipeline` to run inference on a batch of our train data.\n",
        "  - [ ] We need to benchmark inference as well.\n",
        "Here is a good place to talk about prompt-tuning techniques.\n",
        "- [ ] Define the evaluation metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz2qAShQFOO3"
      },
      "source": [
        "## 1. Load a VLM\n",
        "Feel free to explore other VLMs.\n",
        "\n",
        "Here we load a quantized-4bit LLaVA 1.5 7B model.\n",
        "\n",
        "- Awesome resources:\n",
        "    - https://github.com/amrzv/awesome-colab-notebooks\n",
        "    - https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models\n",
        "\n",
        "- Leaderboards:\n",
        "    - https://huggingface.co/spaces/opencompass/open_vlm_leaderboard\n",
        "\n",
        "- Open Source Vision LLMs:\n",
        "    - Paligemma: https://blog.roboflow.com/paligemma-multimodal-vision/\n",
        "    - Zoo 1: https://github.com/salesforce/LAVIS\n",
        "    - Zoo 2: https://github.com/InternLM/InternLM-XComposer\n",
        "    - Zoo 3: https://github.com/OpenGVLab/InternVL\n",
        "    - LLaVA: https://github.com/haotian-liu/LLaVA (1y ago)\n",
        "    - OpenFlamingo: https://github.com/mlfoundations/open_flamingo (1y ago)\n",
        "    - BLIP: https://github.com/salesforce/BLIP (2 ys ago)\n",
        "    - OFA: https://github.com/OFA-Sys/OFA (2 ys ago)\n",
        "    - GIT: https://github.com/microsoft/GenerativeImage2Text (2ys ago)\n",
        "    - DeepSeekVL: https://github.com/deepseek-ai/DeepSeek-VL\n",
        "\n",
        "- Closed Vision LLMs\n",
        "    - OpenAI GPT-4 with Vision: https://openai.com/index/gpt-4-research/\n",
        "    - Claude Vision: https://docs.anthropic.com/en/docs/build-with-claude/vision\n",
        "    - Gemini Vision: https://cloud.google.com/blog/products/data-analytics/how-to-use-gemini-pro-vision-in-bigquery\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Bx8iu9jOssW"
      },
      "outputs": [],
      "source": [
        "# install packages\n",
        "!pip install -q -U transformers==4.37.2\n",
        "!pip install -q bitsandbytes==0.41.3 accelerate==0.25.0\n",
        "!pip install -q datasets\n",
        "!pip install -q evaluate\n",
        "!pip install -q vllm\n",
        "!pip install -q scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YxPK0CPVLl1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import pipeline\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "model_id = \"llava-hf/llava-1.5-7b-hf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD20WLjfXv3G"
      },
      "source": [
        "## 2. Load dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vXu0qlSs4QZ"
      },
      "outputs": [],
      "source": [
        "from datasets import Image, load_dataset\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l2gKnkZseSS"
      },
      "outputs": [],
      "source": [
        "# let's get the train dataset\n",
        "data_path = 'africa_dataset'\n",
        "train_ = load_dataset(\"imagefolder\", data_path, split='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb8ivGq6ewxT"
      },
      "source": [
        "## 2. Run the model with a prepared prompt\n",
        "\n",
        "### i. Use `pipelines`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install tqdm\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNjcvE6SvFqJ"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\"image-to-text\", model=model_id, model_kwargs={\"quantization_config\": quantization_config})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W48r3NxDRskb"
      },
      "outputs": [],
      "source": [
        "# configs\n",
        "max_new_tokens = 200\n",
        "prompt = \"USER: <image>\\nGiven the following classes: {\\\"0\\\": \\\"Text is at the top of the image\\\", \\\"1\\\": \\\"Text is at the center of the image\\\", \\\"2\\\": \\\"Image has no text\\\"}.\\nYour task is to analyse the image, first figure out if there is text. If there is text figure out the position. Finally return only the class key as an int. ASSISTANT: Class key: \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_gYrXxDW9tY"
      },
      "outputs": [],
      "source": [
        "# post processing function\n",
        "import re\n",
        "\n",
        "def process_result(output):\n",
        "  assistant_tag = 'ASSISTANT: '\n",
        "\n",
        "  match = re.search(r'ASSISTANT: (\\d+)', output)\n",
        "  if match:\n",
        "    return int(match.group(1))\n",
        "  \n",
        "  return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qKs2FyELgtkB"
      },
      "outputs": [],
      "source": [
        "# TODO: batch processing*\n",
        "outputs = []\n",
        "\n",
        "prepped_dataset = KeyDataset(train_, \"image\")\n",
        "for out in tqdm(pipe(prepped_dataset, prompt=prompt, generate_kwargs={\"max_new_tokens\": max_new_tokens}), total=len(prepped_dataset)):\n",
        "    outputs.append(process_result(out[0]['generated_text']))\n",
        "\n",
        "train_dataset = train_.add_column('y_hat', outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odOkuQ6q9oTm"
      },
      "source": [
        "## 3. Evaluate your model\n",
        "\n",
        "We are interested in : precision, recall and f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9OXgu_b_B0W"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "precision = evaluate.load(\"precision\")\n",
        "recall = evaluate.load(\"recall\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "targets = train_dataset['label']\n",
        "predictions = train_dataset['y_hat']\n",
        "\n",
        "pr = precision.compute(predictions=predictions, references=targets)\n",
        "rc = recall.compute(predictions=predictions, references=targets)\n",
        "f1 = f1.compute(predictions=predictions, references=targets)\n",
        "\n",
        "print(f'Precision: {pr}\\nRecall: {rc}\\nF1: {f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxnwlBsEyTXl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
